# ğŸ“š Document-Based RAG Chatbot

This project is an end-to-end **Retrieval-Augmented Generation (RAG)** chatbot that allows users to upload documents and ask questions based on their content. It leverages semantic search and LLM reasoning to deliver **accurate and context-aware answers** grounded in your own knowledge base.

---

## ğŸš€ Key Features
- ğŸ“„ Upload your own documents for private knowledge querying
- ğŸ” **FAISS-based vector search** for efficient document retrieval
- ğŸ§  **Query Refinement** to enhance retrieval relevance & reduce hallucination
- âš¡ Fast inference powered by **Groq (Llama 3.1)**
- ğŸ¯ Customizable prompt templates for improved response control
- ğŸ§© Modular architecture â€” easy to extend and optimize
- ğŸŒ Simple & interactive **Streamlit UI**
- ğŸ›  Configurable via `config.yaml`

---

## ğŸ—ï¸ Project Structure

```bash
RagModel/
â”‚
â”œâ”€ api/                          # FastAPI endpoints for RAG pipeline
â”‚  â””â”€ endpoint.py                # Handles user query requests
â”‚
â”œâ”€ config/                       # Configuration & prompt templates
â”‚  â”œâ”€ config.yaml                # Model, embeddings & pipeline config
â”‚  â””â”€ prompt.txt                 # Custom prompt for RAG responses
â”‚
â”œâ”€ core/                         # Core RAG logic
â”‚  â””â”€ model_invoking.py          # LLM invocation & response generation
â”‚
â”œâ”€ data/                         # Local vector DB & temp file storage
â”‚  â”œâ”€ faiss_db/                  # FAISS vector index
â”‚  â”œâ”€ file_hashe.txt             # File hash tracker to prevent re-indexing
â”‚  â””â”€ temp_pdf_store.pdf         # Temporary user document storage
â”‚
â”œâ”€ utils/                        # Utility modules for pipeline support
â”‚  â”œâ”€ custom_memory.py           # Conversation memory management
â”‚  â”œâ”€ db_manager.py              # Vector DB operations
â”‚  â”œâ”€ file_utils.py              # File validation & preprocessing
â”‚  â”œâ”€ load_config.py             # Reads & loads YAML configuration
â”‚  â””â”€ query_refiner.py           # Improves queries before retrieval
â”‚
â”œâ”€ main.py                       # FastAPI backend entrypoint
â”œâ”€ streamlit.py                  # Streamlit UI for user interaction
â””â”€ .env                          # API keys & env variables (not for commit)
````
---

## ğŸ”§ Tech Stack

| Layer | Technology |
|------|------------|
| LLM | Llama 3.1 (Groq API) |
| Embeddings | All-MiniLM-L6-V2 |
| Vector DB | FAISS |
| Backend | FastAPI |
| Frontend | Streamlit |
| Language | Python 3.10 |

---

## ğŸ§  Query Refinement (Key Enhancement!)

Your system includes a **Query Refining Layer** to:
- Expand incomplete or vague questions
- Improve semantic matching with document chunks
- Increase retrieval precision
- Reduce LLM hallucination cases

ğŸ“Œ Implemented in: `utils/query_refiner.py`

Example:
> User asks: *"accuracy?"*  
âœ” Query Refiner â†’ *"What is the model accuracy mentioned in this document?"*

This greatly elevates RAG performance and reliability.

---
## ğŸ§© RAG Architecture
```bash
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚      User Query       â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚   Query Refinement    â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚     Streamlit UI      â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚     FastAPI Backend   â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ RAG Core â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Embeddings â”‚ Vector Store â”‚ Retrieverâ”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
        LLM (Groq - Llama 3.1)
                â†“
            Final Answer

````
---

---

## âš™ï¸ Configuration

All key behavior is controlled through the config files:

ğŸ“ `config/config.yaml`  
ğŸ“ `config/prompt.txt`

Includes settings for:
- Chunking strategy
- Embedding model selection
- API keys
- File persistence & paths
- LLM inference parameters

---

## â–¶ï¸ How to Run

### 1ï¸âƒ£ Install dependencies
```bash
pip install -r requirements.txt
```
---

## 2ï¸âƒ£ Add your Groq API key to .env
```bash
GROQ_API_KEY=your_key_here
```
## 3ï¸âƒ£ Launch the Backend (FastAPI)
```bash
python main.py
```
## 4ï¸âƒ£ Launch the Web UI (Streamlit)
```bash
streamlit run streamlit.py
```
---

## ğŸ§ª Future Enhancements

* Multi-document support

* Advanced long-term memory

* OCR for scanned PDFs

* Cloud deployment with Docker

* Better UI with sidebar file manager
---
## ğŸ¤ Contributing

PRs and suggestions are welcome!
If you find any issues â€” feel free to report them.

---

## ğŸ“„ License

This project is released under the MIT License.
Feel free to use it as a learning resource, a base for your own RAG apps, or internal tools.